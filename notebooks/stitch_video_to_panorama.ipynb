{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce79bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import count_frames\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_FOLDER = os.path.join(ROOT_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9452d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_waltter_path = os.path.join(DATA_FOLDER, \"example_waltter_synchronized.mov\")\n",
    "video_vikture_path = os.path.join(DATA_FOLDER, \"example_vikture_late_15s_synchronized.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d58562",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_left_capture = cv2.VideoCapture(video_vikture_path)\n",
    "video_right_capture = cv2.VideoCapture(video_waltter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_left_frames = count_frames(video_vikture_path)\n",
    "video_right_frames = count_frames(video_waltter_path)\n",
    "\n",
    "print(video_left_frames)\n",
    "print(video_right_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be750aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 6306\n",
    "# total_frames = min(video_left_frames, video_right_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_n_frames = int(video_left_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "right_n_frames = int(video_right_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(left_n_frames)\n",
    "print(right_n_frames)\n",
    "\n",
    "left_width = int(video_left_capture.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "left_height = int(video_left_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "left_fps = video_left_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "right_width = int(video_right_capture.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "right_height = int(video_right_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "right_fps = video_right_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(left_width)\n",
    "print(left_height)\n",
    "print(left_fps)\n",
    "\n",
    "print(right_width)\n",
    "print(right_height)\n",
    "print(right_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fps = 60.0\n",
    "final_height = 1080\n",
    "final_width = 1920*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = cv2.Stitcher_create(cv2.Stitcher_PANORAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c752514",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(DATA_FOLDER, \"example_panorama.mp4\")\n",
    "video_output = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"H264\"),final_fps,(final_width,final_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_histogram(rgb_image):\n",
    "    r_image, g_image, b_image = cv2.split(rgb_image)\n",
    "\n",
    "    r_image_eq = cv2.equalizeHist(r_image)\n",
    "    g_image_eq = cv2.equalizeHist(g_image)\n",
    "    b_image_eq = cv2.equalizeHist(b_image)\n",
    "\n",
    "    image_eq = cv2.merge([r_image_eq, g_image_eq, b_image_eq])\n",
    "    return image_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe84087",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "def apply_clahe(image):\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    image_lab[...,0] = clahe.apply(image_lab[...,0])\n",
    "\n",
    "    bgr_clahe_image = cv2.cvtColor(image_lab, cv2.COLOR_LAB2BGR)\n",
    "    rgb_clahe_image = cv2.cvtColor(bgr_clahe_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return rgb_clahe_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9aeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sift(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    (kps, features) = sift.detectAndCompute(image, None)\n",
    "    kps = np.float32([kp.pt for kp in kps])\n",
    "    return (kps, features)\n",
    "\n",
    "def match_keypoints_and_homography(keypoints1, keypoints2, features1, features2, ratio, reprojection_thresh):\n",
    "    matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "    raw_matches = matcher.knnMatch(features1, features2, 2)\n",
    "    matches = []\n",
    "    \n",
    "    for match in raw_matches:\n",
    "        if len(match) == 2 and match[0].distance < match[1].distance * ratio:\n",
    "            matches.append((match[0].trainIdx, match[0].queryIdx))\n",
    "            \n",
    "    # computing a homography requires at least 4 matches\n",
    "    if len(matches) < 4:\n",
    "        return None\n",
    "    else:\n",
    "        # construct the two sets of points\n",
    "        points1 = np.float32([keypoints1[i] for (_, i) in matches])\n",
    "        points2 = np.float32([keypoints2[i] for (i, _) in matches])\n",
    "        # compute the homography between the two sets of points\n",
    "        (H, status) = cv2.findHomography(points1, points2, cv2.RANSAC, reprojection_thresh)\n",
    "        # return the matches along with the homograpy matrix\n",
    "        # and status of each matched point\n",
    "        return (matches, H, status)\n",
    "    \n",
    "\n",
    "def draw_matches(image1, image2, keypoints1, keypoints2, matches, status):\n",
    "    (hA, wA) = image1.shape[:2]\n",
    "    (hB, wB) = image2.shape[:2]\n",
    "    vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "    vis[0:hA, 0:wA] = image1\n",
    "    vis[0:hB, wA:] = image2\n",
    "    \n",
    "    for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "        if s == 1:\n",
    "            points1 = (int(keypoints1[queryIdx][0]), int(keypoints1[queryIdx][1]))\n",
    "            points2 = (int(keypoints2[trainIdx][0]) + wA, int(keypoints2[trainIdx][1]))\n",
    "            cv2.line(vis, points1, points2, (0, 255, 0), 1)\n",
    "    # return the visualization\n",
    "    return vis\n",
    "\n",
    "def stitch(image1, image2, ratio=0.75, reprojection_thresh=4.0):\n",
    "    \n",
    "    image2, image1 = image1, image2\n",
    "    \n",
    "    keypoints1, features1 = apply_sift(image1)\n",
    "    keypoints2, features2 = apply_sift(image2)\n",
    "    \n",
    "    M = match_keypoints_and_homography(keypoints1, keypoints2, features1, features2, ratio, reprojection_thresh)\n",
    "    \n",
    "    (matches, H, status) = M\n",
    "\n",
    "    if M is None:\n",
    "        return None\n",
    "    \n",
    "    result = cv2.warpPerspective(image1, M[1],\n",
    "        (image1.shape[1] + image2.shape[1], image1.shape[0]))\n",
    "    \n",
    "    result[0:image2.shape[0], 0:image2.shape[1]] = image2\n",
    "    \n",
    "    #vis = draw_matches(image1, image2, keypoints1, keypoints2, matches, status)\n",
    "    #plt.imshow(vis)\n",
    "    #plt.show()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d44bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher.setPanoConfidenceThresh(0.1)\n",
    "stitcher.setRegistrationResol(0.1); # 0.6\n",
    "stitcher.setSeamEstimationResol(0.1); # 0.1\n",
    "#stitcher.setCompositingResol(-1);  # 1\n",
    "#stitcher.setWaveCorrection(True);\n",
    "\n",
    "def rgb_to_gray(rgb_image: np.ndarray):\n",
    "    return np.dot(rgb_image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "#video_left_capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "#video_right_capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = equalize_histogram(image)\n",
    "    image = apply_clahe(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def get_left_mask(image):\n",
    "    \n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    \n",
    "    zeros = np.zeros((height, math.ceil(width/3)*2))\n",
    "    ones = np.ones((height, math.floor(width/3)))\n",
    "        \n",
    "    mask = np.hstack((zeros,ones))\n",
    "    \n",
    "    return mask\n",
    "    \n",
    "    \n",
    "def get_right_mask(image):\n",
    "    \n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    \n",
    "    zeros = np.zeros((height, math.ceil(width/3)*2))\n",
    "    ones = np.ones((height, math.floor(width/3)))\n",
    "        \n",
    "    mask = np.hstack((ones, zeros))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "captured_frames = []\n",
    "\n",
    "for i in range(total_frames):\n",
    "    left_frame = video_left_capture.read()\n",
    "    right_frame = video_right_capture.read()\n",
    "    \n",
    "    left_image = left_frame[1]\n",
    "    right_image = right_frame[1]\n",
    "    \n",
    "    left_image_processed = preprocess_image(left_image)\n",
    "    right_image_processed = preprocess_image(right_image)\n",
    "    \n",
    "    left_mask = get_left_mask(left_image_processed)\n",
    "    right_mask = get_right_mask(right_image_processed)\n",
    "        \n",
    "    #(status, stitched) = stitcher.stitch(images=[left_image_processed, right_image_processed], masks=[left_mask, right_mask])\n",
    "    (status, stitched) = stitcher.stitch(images=[left_image_processed, right_image_processed])\n",
    "\n",
    "    #stitched = stitch(left_image_clahe, right_image_clahe)\n",
    "    #status = 0 if stitched is not None else 1\n",
    "\n",
    "    if status != 0:\n",
    "        print(f\"Image stitching failed: {status}\")\n",
    "        continue\n",
    "        \n",
    "    if status == 0:\n",
    "        plt.imshow(stitched)\n",
    "        plt.show()\n",
    "        captured_frames.append(stitched)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b627a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create(nfeatures=2000)\n",
    "\n",
    "def orb_detect(image1, image2):\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(image1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(image2, None)\n",
    "    \n",
    "    return (keypoints1, descriptors1, keypoints2, descriptors2)\n",
    "    \n",
    "    \n",
    "def match_keypoints(descriptors1, descriptors2):\n",
    "    bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    return matches\n",
    "    \n",
    "def draw_matches_2(image1, keypoints1, image2, keypoints2, matches):\n",
    "    r1, c1 = image1.shape[:2]\n",
    "    r2, c2 = image2.shape[:2]\n",
    "\n",
    "    color_channels = image1.shape[2]\n",
    "    \n",
    "    # Create a blank image with the size of the first image + second image\n",
    "    output_img = np.zeros((max([r1, r2]), c1 + c2, 3), dtype='uint8')\n",
    "    \n",
    "    if color_channels == 3:\n",
    "        output_img[:r1, :c1, :] = np.dstack([image1])\n",
    "        output_img[:r2, c1:c1+c2, :] = np.dstack([image2])\n",
    "    else:\n",
    "        output_img[:r1, :c1, :] = np.dstack([image1, image1, image1])\n",
    "        output_img[:r2, c1:c1+c2, :] = np.dstack([image2, image2, image2])\n",
    "        \n",
    "    # Go over all of the matching points and extract them\n",
    "    for match in matches:\n",
    "        img1_idx = match.queryIdx\n",
    "        img2_idx = match.trainIdx\n",
    "        (x1, y1) = keypoints1[img1_idx].pt\n",
    "        (x2, y2) = keypoints2[img2_idx].pt\n",
    "\n",
    "        # Draw circles on the keypoints\n",
    "        cv2.circle(output_img, (int(x1),int(y1)), 4, (0, 255, 255), 1)\n",
    "        cv2.circle(output_img, (int(x2)+c1,int(y2)), 4, (0, 255, 255), 1)\n",
    "\n",
    "        # Connect the same keypoints\n",
    "        cv2.line(output_img, (int(x1),int(y1)), (int(x2)+c1,int(y2)), (0, 255, 255), 1)\n",
    "\n",
    "    return output_img\n",
    "  \n",
    "    \n",
    "def warp_images(image1, image2, H):\n",
    "\n",
    "    rows1, cols1 = image1.shape[:2]\n",
    "    rows2, cols2 = image2.shape[:2]\n",
    "\n",
    "    list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)\n",
    "    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n",
    "\n",
    "    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n",
    "    list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n",
    "\n",
    "    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n",
    "    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "    translation_dist = [-x_min,-y_min]\n",
    "\n",
    "    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n",
    "\n",
    "    output_img = cv2.warpPerspective(image2, H_translation.dot(H), (x_max-x_min, y_max-y_min))\n",
    "    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = image1\n",
    "\n",
    "    return output_img\n",
    "    \n",
    "def stitch_images(image1, image2):\n",
    "    \n",
    "    (keypoints1, descriptors1, keypoints2, descriptors2) = orb_detect(image1, image2)\n",
    "    \n",
    "    matches = match_keypoints(descriptors1, descriptors2)\n",
    "    \n",
    "    # Finding the best matches\n",
    "    best_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            best_matches.append(m)\n",
    "                \n",
    "    #image_matched = draw_matches_2(image1, keypoints1, image2, keypoints2, best_matches[:30])\n",
    "    #plt.imshow(image_matched)\n",
    "    #plt.show()\n",
    "            \n",
    "    # Set minimum match condition\n",
    "    MIN_MATCH_COUNT = 5\n",
    "\n",
    "    if len(best_matches) > MIN_MATCH_COUNT:\n",
    "        # Convert keypoints to an argument for findHomography\n",
    "        src_pts = np.float32([ keypoints1[m.queryIdx].pt for m in best_matches]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ keypoints2[m.trainIdx].pt for m in best_matches]).reshape(-1,1,2)\n",
    "\n",
    "        # Establish a homography\n",
    "        M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "\n",
    "        result = warp_images(image2, image1, M)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46609a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gray(rgb_image: np.ndarray):\n",
    "    return np.dot(rgb_image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "\n",
    "for i in range(total_frames):\n",
    "    left_frame = video_left_capture.read()\n",
    "    right_frame = video_right_capture.read()\n",
    "    \n",
    "    left_image = left_frame[1]\n",
    "    right_image = right_frame[1]\n",
    "    \n",
    "    left_image_equalized = equalize_histogram(left_image)\n",
    "    right_image_equalized = equalize_histogram(right_image)\n",
    "    \n",
    "    left_image_clahe = apply_clahe(left_image_equalized)\n",
    "    right_image_clahe = apply_clahe(right_image_equalized)\n",
    "    \n",
    "    #plt.imshow(left_image_clahe)\n",
    "    #plt.show()\n",
    "    #plt.imshow(right_image_clahe)\n",
    "    #plt.show()\n",
    "    \n",
    "    left_image_bw = cv2.cvtColor(left_image, cv2.COLOR_RGB2GRAY)\n",
    "    right_image_bw = cv2.cvtColor(right_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    stitched = stitch_images(left_image_clahe, right_image_clahe)\n",
    "    \n",
    "    status = 0 if stitched is not None else 1\n",
    "\n",
    "    if status != 0:\n",
    "        print(f\"Image stitching failed: {status}\")\n",
    "        break\n",
    "        \n",
    "    if status == 0:\n",
    "        plt.imshow(stitched)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread(os.path.join(DATA_FOLDER, \"yosemite1.jpg\"))\n",
    "image2 = cv2.imread(os.path.join(DATA_FOLDER, \"yosemite2.jpg\"))\n",
    "\n",
    "# OpenCV method\n",
    "\n",
    "(status, stitched) = stitcher.stitch(images=[image1, image2])\n",
    "\n",
    "if status == 0:\n",
    "    plt.imshow(stitched)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"OpenCV method: *Stitching failed: {status}\")\n",
    "\n",
    "\n",
    "# Custom method 1\n",
    "    \n",
    "stitched2 = stitch(image1, image2)\n",
    "    \n",
    "status2 = 0 if stitched2 is not None else 1\n",
    "\n",
    "if status2 != 0:\n",
    "    print(f\"Custom 1: Image stitching failed: {status2}\")\n",
    "else:\n",
    "    plt.imshow(stitched2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Custom method 2\n",
    "\n",
    "stitched3 = stitch_images(image1, image2)\n",
    "\n",
    "status3 = 0 if stitched3 is not None else 1\n",
    "\n",
    "if status3 != 0:\n",
    "    print(f\"Custom 2: Image stitching failed: {status3}\")\n",
    "else:\n",
    "    plt.imshow(stitched3)\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_frame = video_left_capture.read()\n",
    "right_frame = video_right_capture.read()\n",
    "\n",
    "left_image = left_frame[1]\n",
    "right_image = right_frame[1]\n",
    "\n",
    "left_image_path = os.path.join(DATA_FOLDER, \"video_left_capture.png\")\n",
    "right_image_path = os.path.join(DATA_FOLDER, \"video_right_capture.png\")\n",
    "\n",
    "cv2.imwrite(left_image_path, left_image)\n",
    "cv2.imwrite(right_image_path, right_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f8337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meow",
   "language": "python",
   "name": "meow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
